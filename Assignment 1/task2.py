# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u5cvRJi1hNGkRJhKIAoXgpYPDJJ_lozo
"""

#from google.colab import files
#uploaded = files.upload()

#!apt-get install openjdk-8-jdk

#!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz

#!tar xf spark-2.4.4-bin-hadoop2.7.tgz

#!pip install -q findspark

#import os
#os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
#os.environ["SPARK_HOME"] = "/content/spark-2.4.4-bin-hadoop2.7"

import json
from operator import add
import sys
import findspark
findspark.init()

#from pyspark.sql import SparkSession

#spark = SparkSession.builder\
#        .master("local")\
#        .appName("Colab")\
#        .config('spark.ui.port', '4050')\
#        .getOrCreate()

#spark


input_file = sys.argv[1]
output_file = sys.argv[2]
numPartitions = sys.argv[3]

from pyspark import SparkContext

sc = SparkContext.getOrCreate()


"""###### default"""



import time

rv_file = spark.sparkContext.textFile(input_file).cache()

reviews = rv_file.map(json.loads).map(lambda x: [x['business_id'],
                                                      x['date'],
                                                      x['review_id'],
                                                      x['stars'],
                                                      x['text'],
                                                      x['user_id'],]).cache()

answer = {}

start_time = time.time()
rev_file = reviews.coalesce(10)


top10_business = rev_file.map(lambda x: x[0]).groupBy(lambda x: x).cache().mapValues(len).sortBy(lambda x: (-x[1], x[0])).take(10)

n_items = rev_file.glom().map(len).collect()
exec_time = time.time() - start_time
answer["default"] = {"n_partition": rev_file.getNumPartitions(), "n_items": n_items, "exe_time": exec_time}

#answer

#answer.clear()
#answer

"""##### Custom"""

n_partitions = int(numPartitions)

start_time = time.time()
custom_rev_file = rev_file.repartition(n_partitions)


top10_business = custom_rev_file.map(lambda x: x[0]).groupBy(lambda x: x).cache().mapValues(len).sortBy(lambda x: (-x[1], x[0])).take(10)

n_items = custom_rev_file.glom().map(len).collect()
exec_time = time.time() - start_time
answer["customized"] = {"n_partition": n_partitions, "n_items": n_items, "exe_time": exec_time}


with open(output_file, 'w+') as outf:
    json.dump(answer,outf)
	
#answer

