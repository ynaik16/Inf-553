# -*- coding: utf-8 -*-
"""new_task1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15pMI8bmfOHCwlS-rJMajNcjBTK0b4-OA
"""
"""
!apt-get install openjdk-8-jdk-headless -qq > /dev/null

from google.colab import drive
drive.mount('/content/drive')

!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz

!tar xf spark-2.4.4-bin-hadoop2.7.tgz

!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-2.4.4-bin-hadoop2.7"



import findspark
findspark.init()
#findspark.init("spark-2.4.7-bin-hadoop2.7")# SPARK_HOME


"""

from operator import add
from pyspark import SparkContext
import sys
from collections import defaultdict
from itertools import combinations
import time
#spark = SparkSession.builder.master("local[*]").getOrCreate()

sc = SparkContext.getOrCreate()


case_number = int(sys.argv[1])
support = int(sys.argv[2])
input_file = sys.argv[3]
output_file = sys.argv[4]
#small_file = "/content/drive/MyDrive/Colab Notebooks/small2.csv"

data = sc.textFile(input_file)

start_time = time.time()

first_line = data.first()
first_line



"""### Case 1

"""

def split_rdd(rdd2):
    new_rdd2 = rdd2.groupByKey().mapValues(set).values()

    return new_rdd2

def user_input(case_number):
  if case_number == 1:
    rdd2 = data.filter(lambda x: x != first_line).map(lambda x: (x.split(',')[0], x.split(',')[1]))
    new_rdd2 = split_rdd(rdd2)
  elif case_number == 2:
    rdd2 = data.filter(lambda x: x != first_line).map(lambda x: (x.split(',')[1], x.split(',')[0]))
    new_rdd2 = split_rdd(rdd2)

  return new_rdd2

#rdd2 = data.filter(lambda x: x != first_line).map(lambda x: (x.split(',')[0], x.split(',')[1]))
#new_rdd2 = split_rdd(rdd2)
#new_rdd2



new_rdd2 = user_input(case_number)

#support = 4
partitions = new_rdd2.getNumPartitions()
#partitions




#s = support // partitions
#s

#chunk = new_rdd2


#count = defaultdict(int)
"""
total_bins = []
for item in chunk:
    total_bins.append(item)
    for i in item:
        count[i] +=1

#total_bins

#count

singles = []
for item1, item2 in count.items():
    if (item2 > s):
        singles.append(set([item1]))

multi_pairs = []

singles

multi_pairs += singles
k = 2

while(len(singles) > 0):
    singles = second_sweep(singles, k , total_bins, s)
    k +=1
    multi_pairs += singles
    
"""

def second_sweep(singles, k , total_bins, s): 
    new_basket = set()
    duples = []
    permutations = combinations(singles,2)
    permutations

    for pair in permutations:
        pair = pair[0].union(pair[1])
        if len(pair) == k:
            new_basket.add(frozenset(pair))
    
    count2 = defaultdict(int)

    for item in new_basket:
        for prev in total_bins:
            if set(item).issubset(prev):
                count2[item] +=1
                if count2[item] == s:
                    duples.append(item)
                    break
    
    return duples

#multi_pairs

def generate_candidates(chunks, support, partitions):
  count = defaultdict(int)
  chunk = list(chunks)
  #chunk = chunks
  s = support // partitions
  
  singletons, total_bins = get_singles(s, count, chunk)
  multi_pairs = []

  multi_pairs += singletons
  k = 2

  while(len(singletons) > 0):
    singletons = second_sweep(singletons, k , total_bins, s)
    k +=1
    multi_pairs += singletons

  return multi_pairs

def get_singles(s, count, chunk):
  total_bins = []
  for item in chunk:
    total_bins.append(item)
    for i in item:
      count[i] +=1

  singles = []
  for item1, item2 in count.items():
    if (item2 >= s):
      singles.append(set([item1]))

  return singles, total_bins

candidates = new_rdd2.mapPartitions(lambda chunks : generate_candidates(chunks, support, partitions))

candidates = candidates.map(lambda x : (tuple(sorted(x)), 1)).reduceByKey(lambda y, z : z).keys().collect()

candidates_sorted = sorted(candidates, key=(lambda x: (len(x), x)))

candidates_sorted

#type(candidates_sorted)

def generate_frequent_itemsets(chunks,candidates_sorted):
  count = defaultdict(int)
  cds = list(chunks)
  similar_items = []

  for item in candidates_sorted:
    for c in cds:
      if set(item).issubset(set(c)):
        count[item] += 1

  for x, y in count.items():
    similar_items.append((x, y))

  return similar_items

"""def get_frequent(count):
  similar_items = []	
  
  for x, y in count.items():
    similar_items.append((x, y))

    
  return similar_items

  """

frequent_itemsets = new_rdd2.mapPartitions(lambda chunks : generate_frequent_itemsets(chunks, candidates_sorted)).reduceByKey(lambda x, y : x + y).filter(lambda z : z[1] >= support).keys().collect()

sorted_frequent_items = sorted(frequent_itemsets, key = (lambda x: (len(x), x)))

sorted_frequent_items

def output_format(out_file, output):
    result = []
    counter = 1

    for x in output:
        if (len(x) == 1):
            result.append("('"+ x[0] +"')")
            
        elif (len(x) == counter + 1):
            a = ','.join(result)
            out_file.write(a + '\n')
            out_file.write('\n')
            counter += 1
            result = []
            result.append(str(x))
        else:
            result.append(str(x))

    a = ','.join(result)
    out_file.write(a + '\n')
    out_file.write('\n')

with open(output_file, 'w') as f:
	f.write('Candidates:' + '\n')
	output_format(f, candidates_sorted)
	f.write('Frequent Itemsets:' + '\n')
	output_format(f, sorted_frequent_items)

end_time = time.time() - start_time

print("Duration: ",end_time)